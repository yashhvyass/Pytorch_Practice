# -*- coding: utf-8 -*-
"""Wine_Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g0RkImFUJ6G3dI1DjTwVEnssjU77Vw-L
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader, TensorDataset

data = pd.read_csv('WineQT.csv')

data.head()

data['quality'].unique()

"""### IMPORTANT"""

class2idx = {
    3: 0,
    4: 1,
    5: 2,
    6: 3,
    7: 4,
    8: 5
}

idx2class = {v:k for k, v in class2idx.items()}
data['quality'].replace(class2idx, inplace=True)

X = data.drop('quality', axis=1)
y = data['quality']

print(f' Features DataFrame representation: \n {X.head()} \n')
print(f'Features Numpy representation: \n {X.head().values}')
print(f'Target DataFrame representation: \n {y.head()} \n')
print(f'Target Numpy representation: \n {y.head().values}')

# Converting to numpy
X = X.values
y = y.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1./3, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

len(set(y_train)), len(set(y)), len(set(y_test))

# Training set features normalization
X_train_norm = (X_train - np.mean(X_train)) / np.std(X_train)
# Convert to Pytorch
X_train_norm = torch.from_numpy(X_train_norm).float()
y_train = torch.from_numpy(y_train)

len(set(y_train)), len(set(y)), len(set(y_test))

len(torch.unique(y_train))

# We can use Tensor Dataset directly or create a custom class for our dataset
train_ds = TensorDataset(X_train_norm, y_train)

torch.manual_seed(42)
batch_size=4
train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
# Just to see few exmamples of the dataloader
for i, batch in enumerate(train_dl):
  if i<1:
    print(f'batch {i}:', 'x: ', batch[0], 'y: ', batch[1])
  else:
    break

# Building a Model
class Model(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super().__init__()
    self.layer1 = nn.Linear(input_size, hidden_size)
    self.layer2 = nn.Linear(hidden_size, output_size)

  def forward(self, x):
    x = self.layer1(x)
    x = nn.Sigmoid()(x)
    x = self.layer2(x)
    # x = nn.Softmax(dim=1)(x) ## softmax = support multiclass classification but cross entropy loss already applies it.
    return x

input_size = X_train_norm.shape[1]
output_size = 6
hidden_size = 64

model = Model(input_size, hidden_size, output_size)

len(set(y_train)), len(set(y)), len(set(y_test))

# Loss
learning_rate=0.01
loss_fn = nn.CrossEntropyLoss()
# Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

X.shape

1143/48

print(train_dl)

# Training
num_epochs = 100
loss_hist = [0] * num_epochs
accuracy_hist = [0] * num_epochs

for epoch in range(num_epochs):
  for x_batch, y_batch in train_dl:
    pred = model(x_batch)
    loss = loss_fn(pred, y_batch.long())
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    loss_hist[epoch] += loss.item()*y_batch.size(0)
    is_correct = (torch.argmax(pred, dim=1) == y_batch).float()
    accuracy_hist[epoch] += is_correct.mean()

  loss_hist[epoch] /= len(train_dl.dataset)
  accuracy_hist[epoch] /= len(train_dl.dataset)

fig = plt.figure(figsize=(12, 5))
ax = fig.add_subplot(1, 2, 1)
ax.plot(loss_hist)
ax.set_title('Training Loss')
ax.set_xlabel('Epoch')
ax.tick_params(axis='both', which='major')

ax = fig.add_subplot(1, 2, 2)
ax.plot(accuracy_hist)
ax.set_title('Training Accuracy')
ax.set_xlabel('Epoch')
ax.tick_params(axis='both', which='major')
plt.show()

# Evaluating the trained model on the test dataset
X_test_norm = (X_test - np.mean(X_train)) / np.std(X_train) ## apply training standardization to test data.
X_test_norm = torch.from_numpy(X_test_norm).float()
y_test = torch.from_numpy(y_test)

pred_test = model(X_test_norm)
correct = (torch.argmax(pred_test, dim=1) == y_test).float()
accuracy = correct.mean()
print(f'Test Acc: {accuracy: .4f}')

"""# Saving the model"""

path = 'wine_classifier.pt'
torch.save(model, path)  ## save both the model architecture and all the learned parameter

"""# Reloading the Model

## If want to save model/architecture as well as parameters.
"""

model_new = torch.load(path)
model_new.eval() # to verify the model architecture

# Check if we are getting the same accuracy.
pred_test = model_new(X_test_norm)
correct = (torch.argmax(pred_test, dim=1) == y_test).float()
accuracy = correct.mean()
print(f'Test Acc: {accuracy:.4f}')

"""## If want to save only parameters."""

path = 'wine_classifier_state.pt'
torch.save(model.state_dict(), path)

model_new = Model(input_size, hidden_size, output_size)
model_new.load_state_dict(torch.load(path))

"""# Possible Error ->

You can come across index out of bound error while training the model. It comes when when don't have that particular target in the data. It can happen because Pytroch inherently works when target ranges between [0,n]. Therefore, if any values are different from this, you want to convert them to [0,n].
"""

