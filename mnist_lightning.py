# -*- coding: utf-8 -*-
"""MNIST_Lightning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1duWZzqrNPjobPQJy6pDGVPgbE-KKM7cq
"""

# pip install pytorch-lightning

import pytorch_lightning as pl
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import MNIST
from torchvision import transforms

from torchmetrics import Accuracy

"""# Setting MODEL"""

class MultiLayerPerceptron(pl.LightningModule):
  def __init__(self, image_shape=(1,28,28), hidden_units=(32, 16)):
    super().__init__()

    self.train_acc = Accuracy(task='multiclass', num_classes=10)
    self.valid_acc = Accuracy(task='multiclass', num_classes=10)
    self.test_acc = Accuracy(task='multiclass', num_classes=10)

    # Make Model
    input_size = image_shape[0] * image_shape[1] * image_shape[2]
    all_layers = [nn.Flatten()]
    for hidden_unit in hidden_units:
      layer = nn.Linear(in_features=input_size, out_features=hidden_unit)
      all_layers.append(layer)
      all_layers.append(nn.ReLU())
      input_size=hidden_unit

    all_layers.append(nn.Linear(hidden_units[-1], 10))
    self.model = nn.Sequential(*all_layers)

  def forward(self, x): ## simple forward pass which return the logits. Output of the last fully connected layer, before applying softmax layer.
    x = self.model(x)
    return x

  def training_step(self, batch, batch_idx): ## executed on each batch during training.
    x, y = batch
    logits = self(x)
    loss = nn.functional.cross_entropy(self(x), y)
    preds = torch.argmax(logits, dim=1)
    self.train_acc.update(preds, y)
    self.log("train_loss", loss, prog_bar=True)
    return loss

  ## executed at the end of each training epoch.
  ## we log training acc after each epoch because then plot can become too noisy
  def on_train_epoch_end(self):
    self.log("train_acc", self.train_acc.compute())
    self.train_acc.reset()

  def validation_step(self, batch, batch_idx):
    x, y = batch
    logits = self(x)
    loss = nn.functional.cross_entropy(self(x), y)
    preds = torch.argmax(logits, dim=1)
    self.valid_acc.update(preds, y)
    self.log("valid_loss", loss, prog_bar=True)
    return loss

  def on_validation_epoch_end(self):
    self.log("valid_acc", self.valid_acc.compute(), prog_bar=True)
    self.valid_acc.reset()

  def test_step(self, batch, batch_idx):
    x, y = batch
    logits = self(x)
    loss = nn.functional.cross_entropy(self(x), y)
    preds = torch.argmax(logits, dim=1)
    self.test_acc.update(preds, y)
    self.log("test_loss", loss, prog_bar=True)
    self.log("test_acc", self.test_acc.compute(), prog_bar=True)
    return loss

  def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
    return optimizer

"""3 steps to set up dataloaders fror lightning

1. make the dataset part of the model.
2. set up the data loaders as usual and feed them to fit method of a lightning Trainer.
3. Create a LightningDataModule - used below

# Setting DataLoaders
"""

from torch.utils.data import DataLoader, random_split
from torchvision import transforms
from torchvision.datasets import MNIST

class MnistDataModule(pl.LightningDataModule):
  def __init__(self, data_path='./'):
    super().__init__()
    self.data_path = data_path
    self.transform = transforms.Compose([transforms.ToTensor()])

  def prepare_data(self): ## general steps such as downloading the dataset
    MNIST(root=self.data_path, download=True)

  def setup(self, stage=None): ## define datasets for training, validation and testing
    # stage is either fit, validate, test or predict
    mnist_all = MNIST(
        root=self.data_path,
        train=True,
        transform=self.transform,
        download=False
    )

    # mnist doesn't have a dedicated validation split, that's why we use random_split function to divide 60,000 samples.
    self.train, self.val = random_split(mnist_all, [55000, 5000], generator=torch.Generator().manual_seed(1))

    self.test = MNIST(
        root=self.data_path,
        train=False,
        transform=self.transform,
        download=False
    )

  def train_dataloader(self):
    return DataLoader(self.train, batch_size=64, num_workers=4)

  def val_dataloader(self):
    return DataLoader(self.val, batch_size=64, num_workers=4)

  def test_dataloader(self):
    return DataLoader(self.test, batch_size=64, num_workers=4)

torch.manual_seed(42)
mnist_dm = MnistDataModule()

"""Lightning data module implements a Trainer class that makes the training model super conveninent as it takes care of the intermediate steps such as zero_grad(), backward(), optimizer_step().

We can also use one or more GPUs.

# Training Model
"""

mnist_classifier = MultiLayerPerceptron()

if torch.cuda.is_available():
  trainer = pl.Trainer(max_epochs=10, gpus=1)
else:
  trainer = pl.Trainer(max_epochs=10)

trainer.fit(model=mnist_classifier, datamodule=mnist_dm)

"""# Evaluating the Model using TENSORBOARD

1. I we are running the code multiple times, Lightning will track them as separate folders.
2. Lightning allow us to load a trained model and train it for additional epochs conveniently.
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir lightning_logs/

"""We can train the model for more epochs and it's results will show as the next version."""

